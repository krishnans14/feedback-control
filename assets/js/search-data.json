{
  
    
        "post0": {
            "title": "Feline Classifier",
            "content": ". Intro . Feline Classfier: Cheetah or Leopard or Jaguar . This program is to create an image classifier which will help in solving the age-old conundrum I have about distinguishing the three felines: Cheetah, Leopard and Jaguars. I want to see if a simple deep neural network can do a better job than me (who has just now learned to see the difference between Cheetah and Leopard). . The question has been in mind ever since I heard stories of leopards (or panthers) in the university campus where I did my masters. I eventually never saw one, but I caught myself consistently misusing the names (including cheetah as well). The whole confusion stopped only when I left the campus because I no longer was worried about the semantics of the animal which can attack me when I&#39;m walking through the campus at 2 AM. (This is a reflection from my time at IIT Bombay, India, where the campus is known to get leopard visits) . What&#39;s the point? . There is no point, like there is no point in life. . I want to learn the process of a machine learning product development: From data collection to production. . How am I doing it? . Training/Validation Data: Bing Image Search through Azure (150 images per group) | Neural Network Training: Resnet18 (Neural net) with Fastai (PyTorch) | Google Colab for the GPU resource | Binder for the production version | . Who gave this insane Idea? . The feline classifier? That&#39;s mine. I will patent it if I can. But well, what&#39;s the point? . You mean developing an ML product? You have to blame Jeremy Howard of the Fast.ai team. He is a nice guy. . So you did everything on your own? . Ha ha, no way. I&#39;m just following the Chapter 2 of Jeremy&#39;s Fastai book. . You did not get any other ideas? . Glad you asked. I also tried a few more. I developed a classifier to distinguish between cricketers (bowlers, batsmen) and baseball players (pitchers, batters). (I&#39;m from a former British colony, no points for guessing that). But the training data was pretty disappointing and I ended up spending too much time battling with that. It is available here: Cricball-classifier . Step 0: Get the libraries . Let&#39;s get started. I would install and set up the fastbook. This code is practically an adaptation of 02_production.ipynb code. . import fastbook fastbook.setup_book() . And get the vision components from fastai library. . Now, don&#39;t come with all guns blazing against the . import * . go and talk to Jeremy. But he has assured me that there won&#39;t be much namespace pollution. Anyway, what&#39;s to worry about namespace pollution in a Jupyter notebook anyway? . from fastbook import * from fastai.vision.widgets import * . Step 1: Collecting training data . I did not think this would be so confusing. In the video, Jeremy just says go and get your microsoft azure account and then get the key and you can use them from the Jupyter notebook to get the image file. . This was the moment I realized how much the world has changed since I seriously did some internet treasure hunting. I was an early pioneer in this kind of exploration, I even had a website back in 2002 using my UG university, but off late, the cloud and others have been going at breakneck speed that I felt like a child lost in a festival crowd. . So I was aimlessly wandering around in my Microsoft Azure account, not knowing what I&#39;m doing. And I even confused my unique USERID as the API key and then used it to find the API scolding me as being stupid. . Anyway, I got a saviour in the form of a guy who was an Azure expert who joined a conference call to learn Pythong together. He took me through the tangled maze. The idea is something like this (I actually have made it super simplified and not sure if the following steps are enough. In fact, recently Microsoft moved the image search API elsewhere and so I&#39;m not even how it will work now). . Create an Microsoft Azure account (if you have a Microsoft account, use it). The account is free, though, after 30 days, you will have to subscribe to an account which will be &#39;pay as you go&#39; basis. However, the image search that we do will be within the Free tier and we don&#39;t have to pay anything. | Go to your portal | Create a resource (+) | Bing Search API | Once you created the API, you should check your keys under &quot;Keys and Endpoints&quot; column of RESOURCE MANAGEMENT. | . There are two keys available, and which is also suggested to be changed regularly. Any one will do. . key = &#39;abcdx2123dasfk4353m434&#39; . Now, to actually collect the data. . The first step here would be to name the types of images are trying to get (which would also form the basis for our image search) . image_types = &#39;cheetah&#39;,&#39;leopard&#39;,&#39;jaguar&#39; path = Path(&#39;feline_classifier&#39;) . The following code will do it. Some points to note: . If the image search keyword we use would have a space (e.g. African Cheetah), then a separate image_folder_name variable should be created such that the latter contains foldernames that are acceptable. In the following, (path/type), refers to the use of the image_types variable to create a folder name. So if there is a space in the folder name, it may not be ideal to manipulate them. | There are two fastai functions used in this code (the following are the actual source codes from the fastbook module): search_images_bing(key, term, min_sz=128) . Which calls the appropriate API (api.cognitive.microsoft.com) with the key, and with a image search query for 150 images (that&#39;s the limit imposed by Microsoft, it seems). And then the function download_images(), which downloads images listed in text file url_file to path dest, at most max_pics | . download_images(dest, url_file=None, urls=None, max_pics=1000, n_workers=8, timeout=4) . Neat, isn&#39;t it? . if not path.exists(): path.mkdir() for type in image_types: dest = (path/type) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{type}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . We can check whether the files have been download. Before we use these image files for training the model, it might be a good idea to actually verify if any of the images were just some dead links. Fastai has a function that does it: verify_images(fns), which verifies if any of the files in the input list can not be opened. To create this input list, we use the function get_image_files(path) which will return all the image files present in the subfolders of the path (by recursively going into everything). . fns = get_image_files(path) failed = verify_images(fns) . We can check how many of the files that were not opening properly by looking at the failed variable. To remove those invalid files, we can unlink them using the map() method . failed.map(Path.unlink) . (#11) [None,None,None,None,None,None,None,None,None,None...] . Now, the data collection process is done . Step 2: Training . Before we use the data to train a neural net, this data should be assembled in a particular form. In fastai these are called Dataloaders . Raw data --&gt; Dataloaders . The data that we have got from Bing image search requires some form of organizing, the least of which is to split them into training and validation components. The other actions include, how to get the image files, how to obtain the labels, how to preprocess the images (since images are resized usually to a standard (224px) form for training neural nets), etc. This is handled in fastai using the DataBlock class. . felines = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), # says 20% validation data get_y=parent_label, item_tfms=Resize(128)) # The type of transform to perform: Resize. . felines here correspond to an instance of the class DataBlock that we will manipulate for our purpose. The key element here for our purpose would be the item_tfms which can take many forms. When the Resize option did not have any specific input, it would just resize the entire image to the size required. We could also add the ResizeMethod option inside the Resize function to make it do things differently. . ResizeMethod.Squish : squish the image file | ResizeMethod.Pad (with the additional pad_mode= option) : Add empty padding to the edges of the image file | . We can also replace Resize. In this case, RandomResizedCrop (with the min_scale= option): Randomly resize the image by cropping different parts of the same image. . In theory, RandomResizedCrop is better because it takes one through a lot of different cuts to understand the image under study. But I&#39;m skeptical if this is true in case of the feline detection as the patterns on the body of the felines might be viewed to be the same. On the other hand, the face of cheetah and leopard can be easy to distinguish by the neural net. . One advantage of the RandomResizedCrop for our case is that it will create more input data from the 150 odd that we have. Yay! This process is called Data Augmentation. And with all data in the same size, running them in a batch mode on GPUs become easy ( now we know, like wheat enslaved us to become farmers, GPUs are enslaving us to take photographs in a specific format. Huh, colonialists!! &lt; sarcasm&gt;). To do this, we add another parameter/option to the class called aug_transforms . We can change the item_tfms of the class after initialisation by using the new method associated with the class, i.e. felines.new(item_tfms=). . felines = felines.new(item_tfms=RandomResizedCrop(224,min_scale=0.5), batch_tfms=aug_transforms() ) . Once we have ensured that the class felines contain all that we want, we then apply the dataloaders method of the class on the data of our interest to create a dataloader object . dls = felines.dataloaders(path) . Now, we can happily go and train out neural net. Yay. So which neural net are we going to use? . ResNet-18 . Woohoo! That sounds big. Is it? What is it? Well, it is a pre-trained PyTorch Convolutional Neural Network model for residual learning of images. . Wo! Wo! hold on. Too may jargons. . Convolutional Neural Network (CNN) - For now, I just know that they are pretty good with images. | pre-trained - It appears that we don&#39;t need to train a neural net from scratch when it comes to images. Even though some images are completely, utterly, unrelated, even then some basic pretraining on a vast image database appears to be better than starting from scratch. The way ResNet-18 is trained, etc., are for another time (clue: residual learning). For now, we are going to work on top of somebody else&#39;s work, even if they may have never tried to answer the most difficult question of all, &quot;Is it jaguar or leopard?&quot; | . This appears one of the very good examples of Sitting on the shoulders of giants . We will use the cnn_learner function under fastai which takes in a dataloader and an architecture . cnn_learner(dls, arch, loss_func=None, pretrained=True, cut=None, splitter=None, y_range=None, config=None, n_out=None, normalize=True, opt_func=&lt;function Adam at 0x000001DDAE66C670&gt;, lr=0.001, cbs=None, metrics=None, path=None,model_dir=&#39;models&#39;, wd=None, wd_bn_bias=False, train_bn=True,moms=(0.95, 0.85, 0.95), ) . Our interest lies mostly on the components in the first line. The architecture we would use would be ResNet18 and we will use a metric that is error_rate . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(2) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.686050 | 0.805748 | 0.250000 | 00:14 | . epoch train_loss valid_loss error_rate time . 0 | 0.795215 | 0.528109 | 0.154762 | 00:15 | . 1 | 0.590685 | 0.502110 | 0.142857 | 00:15 | . learn is an instance of a Learner class under fastai which contains several methods, and the first of which we use is fine_tune(), which essentially is to tune the model. The one parameter it takes in is the epoch which is the number of times we would ask the algorithm to look at each image. We use fine_tune() instead of fit() because we are starting with a pre-trained model (and so not to lose all those nice parameter values already obtained by someone else, to start our model tuning). Here, we are asking for an epoch of 2, but we could do more as well. . Once the model is tuned, we have to see how good it performed in the validation data set. We go for the confusion matrix based visualization of the performance. We go through two steps for this. First, we create an instance of the fastai class, ClassificationInterpretation&#39;s from_learner method which is an interpretation object. This interpretation object contains the plot_confusion_matrix() method which will let us know how the classification went. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . The same interpretation object contains other useful analysis information about the model we have trained and its performance. One is the plot_top_losses(x, nrows=) method, which can plot the images where the model performed really bad. We can ask the method to print a top x number of images, and splitting the visualization in nrows. . interp.plot_top_losses(10, nrows=2) . Step 3: Retraining . One advantage of doing simple image classification is that, we can use our eyes to judge how bad things are (okay, I know, I still can&#39;t distinguish between jaguar and leopard, but I&#39;m talking about in general, for example, in the cricball classfier, I was absolutely great in distinguishing between a pitcher and bowler or a batsman and a batter). And top losses would let us do that. It is also indirectly a way to clean the data, because we did not do any check on whether the labels were right. I mean, after all, bing image search doesn&#39;t come with any warranty that images correspond to the keyword (If I was a lawyer, I would check if one can file a class action suit against Microsoft for images that have been wrongly labeled. Fortunately, I&#39;m not a lawyer and I don&#39;t live in the US). . As you may guess, fastai has a way to clean the data and unsurprisingly, it is called ImageClassifierCleaner. It is a class which provides a widget that helps to clean the images on the learner obtained from a cnn_learner training. The Widget lets one to either delete the image or provide the right label if it has been wrongly labeled (I promise, I won&#39;t touch the jaguar vs leopard mistakes) across both the training and validation data. . cleaner = ImageClassifierCleaner(learn) cleaner . Once we do the cleaning by using the widget, we make them work by unlinking the images set of deleting and changing the paths of the images whose labels were wrong. . In this analysis, I found so many Jaguar cars and had to delete them . for idx in cleaner.delete(): cleaner.fns[idx].unlink() for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . The above may throw an error, for example, if there are no images marked for deletion. Though that&#39;s not going to do much damage, just comment that line and run the other line. . Once we clean up the data, we have to rebuild the dataloaders and rerun the model fine tuning. Let&#39;s remember to keep the seed (42 is chosen for a reason, it is the answer to life universe and everything). . felines = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) felines = felines.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms() ) dls = felines.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(3) # This, time, one more fine tuning level interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . epoch train_loss valid_loss error_rate time . 0 | 1.639320 | 0.572289 | 0.190476 | 00:15 | . epoch train_loss valid_loss error_rate time . 0 | 0.674895 | 0.371915 | 0.166667 | 00:15 | . 1 | 0.592105 | 0.370409 | 0.083333 | 00:16 | . 2 | 0.499299 | 0.344877 | 0.095238 | 00:15 | . Does the confusion matrix got better? If not check again the top losses and go through the same retraining steps a bit. But if I do it so many times, I might end up populating the AI with my own crooked intellegence. So perhaps one or two times would be enough. . Step 4: Inference using Model and Export for Deployment . The fine tuned model is now in learn. We can export it into a format such that it can be reused without someone having to retrain the model, etc. That is, while we would need GPUs for training (cnn_learner()), we do not actually need GPU resources for running the prediction component of the model. For this purpose, we will export the model into a pickle format. . The pickle module implements binary protocols for serializing and de-serializing a Python object structure. . learn.export() . We should now have a &#39;export.pkl&#39; file in the current folder which can be checked as follows, . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . This pkl file can be taken wherever we want, and if we have the appropriate tools, we can then use it to create an application. As a first step, we will create an application within the ipython notebook. To simulate how it will happen in any ipython notebook, we load the pickle file (called unpickling) into an object which we will use for inference. . learn_inf = load_learner(path/&#39;export.pkl&#39;) . To test whether a given image is a specific type of feline we just have to use the predict method and specify the image location . dest = &#39;jaguar1.jpg&#39; link_feline = &#39;https://l-express.ca/wp-content/uploads/2020/06/Jaguar-Bresil-1024x576.jpg&#39; download_url(link_feline, dest) im = Image.open(dest) im.to_thumb(128,128) . dest = &#39;jaguar2.jpg&#39; link_feline = &#39;https://upload.wikimedia.org/wikipedia/commons/e/ee/Jaguar_full.jpg&#39; download_url(link_feline, dest) im = Image.open(dest) im.to_thumb(128,128) . im = Image.open(dest) im.to_thumb(128,128) . learn_inf.predict(&#39;jaguar2.jpg&#39;) . (&#39;jaguar&#39;, TensorImage(1), TensorImage([1.6511e-04, 9.2817e-01, 7.1662e-02])) . !zip -r ./feline_images.zip ./feline_classifier/ . The output contains three components . The name as in the folder name or the keyword used | The tensor number, that is the position of the inferred location in the vector/array (tensor is the PyTorch way of saying array) | The list of probabilities corresponding to each class. | . Since I worked on google colab, I had to download the file to my PC and then move it wherever I want (even within my own PC which doesn&#39;t have a GPU resource) . from google.colab import files files.download(&#39;export.pkl&#39;) . What do we need to create an application . iPython widgets | Voilà | ipython widgets bring Java Script + Python to web browsers. So we can run an application with jupyter notebooks . Voilà takes us one step further and eliminates the need for a jupyter notebook as well where the end user can access the application with just a browser. . Step 5 Creating a Notebook App . The code in this section can be deployed separately in any Jupyter notebook . First, we need a file upload widget through which we can upload an image file into the app . btn_upload = widgets.FileUpload() btn_upload . This would give us a widget clicking which we can feed in the backend with an image file (or whatever as the case maybe) . img = PILImage.create(btn_upload.data[-1]) . We can then use Output widget to display the uploaded image . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . The predictions can then be obtained through the inference. Remember that we assume we already have the export.pkl file. If not we upload it. In the following, we use the google colab to do it. . from google.colab import files files.upload() . Funnily enough, the above mechanism (uploading a file) is what we are trying to replicate. Now, we then load the pkl file . learn_inf = load_learner(path/&#39;export.pkl&#39;) . Since we already have it in our current folder, we don&#39;t do it and directly go to prediction . pred, pred_idx, probs = learn_inf.predict(img) . And we use Label to display the results . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . We can also add a button to classify, kind of separting the uploading and the classifying process . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . Since we havea button, then we also need a way to handle the event of clicking the button. So we will put together above steps (predict, display, etc.) into the event handler . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . Now, we can put all these together (in a vertical box called VBox) . btn_upload = widgets.FileUpload() VBox([widgets.Label(&#39;Select your Feline!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Step 6: Developing a Real App . For this step, there is nothing new in terms of the code, we just need to put together everything in Step 5, put them in a new jupyter notebook and add them into a single folder. . Then, install voila, if not already there, and then connect voilà to the current jupyter notebook. . The final uploaded model on Binder is accessible here: .",
            "url": "https://krishnans14.github.io/feedback-control/projects/2020/12/01/Feline-Classifier.html",
            "relUrl": "/projects/2020/12/01/Feline-Classifier.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Cricket vs Baseball",
            "content": "This is a deep learning trained model that attempts to distinguish between cricket and baseball players . The result will be either of: Batsman (cricket) Bowler (cricket) Batter (baseball) Pitcher (baseball). . The model was trained using image search data from Bing. However, the data was quite bad (with a lot of cartoons and unrelated images) and hence the classifier is not very good. For example, the stumps in a cricketer&#39;s image is primarily attributed to a batsman and if stumps appears in a bowler&#39;s image, it is likely to be classified as batsman. . This is the reason why I moved to handling another problem, Feling Classifier. That blog entry details the step-by-step discussion on how the neural network classifier was developed using Fastai, PyTorch and Bing Image search . In this blog entry, a very brief description of the steps that went into building this model is given, espcially to both show how we collected the images as well as the reason why this wasn&#39;t much successful by showing examples of the training data. The steps are cursory and some outputs (such as training) are not shown here (I was trying to reduce the size of the jupyter notebook and instead ended up removing all outputs). . Even then, the app was built and is hosted on binder here: . Acquiring Images using Bing Search . key = &#39;xxpas324sc4r3r32&#39; # This is not the actual key, but something random to hide my key . To ensure that the images are as relevant as possible, I used a search term with the name of the sport prefixed to it. This means, I had to use a different image folder name. . image_types = &#39;cricket batsman&#39;,&#39;cricket bowler&#39;,&#39;baseball batter&#39;, &#39;baseball pitcher&#39; image_folder_names = &#39;batsman&#39;, &#39;bowler&#39;, &#39;batter&#39;, &#39;pitcher&#39; path = Path(&#39;cricball&#39;) . if not path.exists(): path.mkdir() for type,foldername in zip(image_types,image_folder_names): dest = (path/foldername) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{type}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . And remove any corrupted images . fns = get_image_files(path) failed = verify_images(fns) failed.map(Path.unlink); . sportsmen = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = sportsmen.dataloaders(path) . Training the Model to pick the bad ones . sportsmen = sportsmen.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = sportsmen.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . interp.plot_top_losses(18, nrows=3) . . Honestly, this is disappointingly bad set of data. So many cartoons and sketches, and even some completely irrelevant images. So we clean these up a bit. . cleaner = ImageClassifierCleaner(learn) cleaner . Unfortunately, the widget is not showing up above, but well, you can refer to how it looks in the Feline Classifier blog entry. . for idx in cleaner.delete(): cleaner.fns[idx].unlink() for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . Running the traning again . sportsmen = sportsmen.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = sportsmen.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . interp.plot_top_losses(18, nrows=3) . . Slightly better, but the mismatch in the prediction says that NN is struggling in its task. But the confusion matrix seems okay here. And we say we will stop this whole thing here. . So we just export the model and live with it . learn.export() . The Final App . The app is developed in the same way as Feline classifier and is available in this github repository: Cricket vs Baseball . I still think it works decently and the app is hosted on Binder and you can access it here: . I will conquer this problem better another time .",
            "url": "https://krishnans14.github.io/feedback-control/projects/2020/12/01/Cricket-Versus-Baseball.html",
            "relUrl": "/projects/2020/12/01/Cricket-Versus-Baseball.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Project SPHEREAU",
            "content": "powered by fastpages . Background . The project SPHEREAU, a French abbreviation (Solutions de Programmation Hiérarchisée pour l’Efficience des Réseaux d’EAU), is a French government funded collaborative project guided by the HYDREOS group. This project brought together the following industry and academic groups to develop a method to bring more energy efficiency to water distribution networks: . Université de Lorraine (CRAN lab) | ENGEES (Strasbourg) | SAUR | TLG Pro | IRH Ingénieur Conseil (ANTEA Group) | . My post doctoral research work was funded by this project. The work packages in which our team (CRAN) was involved in were: . Data-based model development | Leak estimation | . Main Challenges . Fairly straightforward, one may think. Throw whatever sensor data available to a machine learning algorithm, it will spit out a model. Then do some anomaly detection to get the leaks. Done. . . Warning: Not so fast when you deal with real-world data . Of course, real-world data has problems. Missing data, some mis-labeled data and so on are the bread and butter of data scientists. But, there was more: . Only operational data was available. Meaning, we can&#39;t train a model to understand normal behaviour and leaky behaviour. | No ground truth and no labeling of leaks or other anomalies. | The Sensor data could be missing for prolonged periods. And there were multiplicative and drift errors in Sensor data. | . Important: Explain your predictions . What&#39;s the idea to describe it here? . I want to see this as an example of how traditional control theory discipline can aid in the use of machine learning tools for engineering applications. We made use of civil engineering domain background, graph theory, some good old linear algebra to augment the Machine learning model to create a nice recipe for an explainable estimation of the leaks. . This I think is how a Post-Modern-Control-Engineer (or PoMoCoE) be pacified by the use of his skills in aiding the advancing Machine learning Tsunami and not be washed away by it. . And it also clearly shows that almost all the skills a control engineer holds are transferable with the integration of Machine Learning work flow. . Exploratory Data Analysis and Feature Selection . Understanding available Sensor Data . The data available from the sites were from the following sensors: . Flow sensors | Tank Level sensors | Several other quality analysis sensors | . From the perspective of leak detection, the water quality sensors aren&#39;t useful and hence we focus only on the flow and tank sensors. The sensors were available at sampling rates between 15 minutes and 1 hour (not counting missing data) depending upon when/where a sensor is installed. . We had to hunt for sensors for which data was available over a long period of time. And perhaps, data available for a group of sensors topological close to each other (and hence connected). After analyzing 100+ sensors we focused on a few of them to develop and illustrate the approach and then generalize it. . During this data analysis, we made note of a number of things that were not explicit based on what the technicians could share with us: . Some sensor data had a weird inverted characteristics and the data needed some pre-processing to even make it look reasonable. | Data resolution capabilities of some sensors were not good and led to an oscillatory behaviour with sensor data oscillating between two quantization states. | Sensors appeared to have been changed during the period for which the data was available and displayed different characteristics. | . These aspects further reduced the sensors that we focused on. . Feature Selection/Engineering . Target: Flow Features available: Time stamp We are modelling the consumption characteristics of villages and groups of villages and are using the flow sensors (and if available, level sensors). Modelling it purely as a time-series data isn&#39;t viable because of the following challenges: . Nonlinear characteristics of the data | Presence of unknown disturbances (Sensor faults, Leaks) | Presence of systemic changes (Addition of new water consumers, characteristic change due to vacations, etc.) | . We first started looking for features that we can help in modelling the behaviour of the water network users. These can be split into three categories (apart from that of data): . The temporal features | The meteorological features | The social features | Temporal Features: Extracting features from the time stamp associated with the sensor data is the obvious first step. . Time of the day | Day of the week | Weekend or Weekday | . | Meteorological Features Using some publicly available data sets, we collected a number of meteorological features: . Temperature (per day: average, minimum, maximum ) | Humidity (per day: average) | Precipitation (day sum) | . | Social Features Again, using some publicly available details about France and in particular the region GrandEst where the villages are situated, we gathered the following features: . Holidays | School vacations | . | Post-Exploratory data analysis . Instead of having two separate features as day of the week and weekend/weekday, we did what is referred to as feature engineering. We numbered the days of the week from 1 to 5 for the week days and 14 and 15 for the week ends. | Only the temperature data from the weather features were retained. | There were other social details we wanted to integrate (such as the &#39;Bridge&#39; vacation that the French routinely take by taking a day off on Friday/Monday when there is a holiday on Thursday/Tuesday respectively). However, this was abandoned after some initial attempts. | Building a Machine-learning model . Model selection and constraining . The choice of model was tricky. Given that we are relying on features instead of inputs, the model, a blackbox machine learning model made sense. The following constaints drove the direction of choosing the model: . Nonlinear characteristics | Lack of ground truth | Presence of outliers in the training data (leaks, sensors faults) | . The lack of ground truth ruled out a neural network model. An SVM model has the capability to not worry too much about outliers and has the capability to learn any nonlinearity if an appropriate kernel is chosen (Radial Basis Function - RBF). To this, we decided to throw in another comfort . Constraining the model through domain knowledge | . That is, instead of an automatic learning algorithm to learn the hyperparameters and the parameters of the model, the hyperparameters were fixed. This would seem the solution is less generic (true), but would however work better for this specific case (also true). . . Note: The winner is: A hyperparameter-constrained SVM-Regression with RBF Kernel . Model training . With all the above set up, the training of the model run smoothly. To avoid overfitting, a regularisation kernel is also computed and is used as part of the model training. The key sequences can be summarized as: . The data and the characteristics are preprocessed to give out neatly manageable inputs and ouputs. sensor_data, features = preprocessing(raw_data, characteristics) . | The Kernels are then computed using the model constraints (by fixing the hyperparameters, rbd_nodes and rbd_sigma) kernel, kernel_reg = computer_kernel_rbf(features, rbf_nodes, rbf_sigma) . | With the Kernels in place, we just run a simple regularized linear regression to obtain the kernel parameters kernel_parameters = calculate_kernel_parameters(kernel, kernel_reg, sensor_data) . | Once we have the model, we predict the flow data such that we can then use it to generate simulated data . Topological model of the Water Network . Why do we need topological model? Because, we need to find a way to distinguish between the leaks and sensor faults using the relationship between the sensors due to their positioning. . Graph equivalent through Electrical analogy . We first developed a simple electrical circuit equivalent for the sensor network. The flow sensor would be replaced by current sensors, the consumption points would be replaced by resistances (connected to ground). (The capacitances would help modelling the water tanks, but this was not included in the results). . A graph equivalent model was derived from this electrical equivalent circuit with a novelty, where the flow or the measurement is represented by edges and the consumption points represented by nodes. This is quite contrary to the traditional modelling approach, but served well for our purpose. . We then used this graph and added representations for leaks ($ mathcal{L}$) and sensor faults ($ mathcal{D}$). Both these anomalies were represented as edges with appropriate colouring. The transformation from the WDN to the graph to analyse for the next step is given in the following images . . . . . Note that, we are assuming in Fig 5 that all the nodes have a leak and sensor faults. This is not necessarily the case in practice (and we will come to that), but is shown just for the sake of illustration. . Simultaneous detectability of Sensor faults and Leaks . Given this monstrous looking graph obtained from a simple WDN, we try to understand if we can derive conditions to distinguish between a fault and a leak and when. . Note: We prove that this is under a specific graph condition which is pretty easy to check (partition the graph into subgraphs, and verify if the subgraph containing the node 0 is a tree). Under what conditions? Assuming that we have a single measurment (from all sensors) and want to distinguish between sensor faults and leaks. . How is it useful? . The Leak Estimation Strategy . Given a large water distribution network and its graph, we partition into several subgraphs. This is necessitated because on large networks, the amplitude of flow measured by sensors sitting on top of trees can become insensitive to leaks and faults in sensors downstream. . This allows us to analyze sub-regions of a water distribution network and apply the simultaneous detectability results. . Consider all possible leak and sensor fault combinations | Obtain those combinations that are distinguishable using the detectability results | This is an offline process and needs to be done once. And for online, . Solve a simple linear equation Ax = b (x contains both leaks and sensor faults) | Apply positivity constraints on leaks | Compute the $ mathbb{L}_1$ norm | Output: The combination(s) of leaks and faults presenting the smallest norm. | This algorithm essentially reproduces LASSO, with one extra advantage. Since several combinations can have the smallest norm (it happens more than we imagine, especially when there are sensor faults), we can estimate leaks not as a single point, but as a range. This would help the engineer to make a better decision and explains the underlying process better. . Concluding Remarks . Our paper has been submitted for review and the draft version can be accessed in this archive link arXiv:2007.09401 .",
            "url": "https://krishnans14.github.io/feedback-control/projects/2020/11/30/Project-SPHEREAU.html",
            "relUrl": "/projects/2020/11/30/Project-SPHEREAU.html",
            "date": " • Nov 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "About This Blog",
            "content": "powered by fastpages . What&#39;s this blog about? . My name is Krishnan Srinivasarengan and I am a post-modern-control-engineer. . . Warning: There is no such a thing as post-modern-control-engineer . True. I made it up. But for a reason. This blog is about my reactions and responses to the transition happening around the world and their relevance to the domain of control theory and applications. . Who am I to write this? . I have a PhD in Automatic Control. I graduated from the Université de Lorraine in 2018 having done my research under the auspices of the lab, &#39;Centre de Recherche en Automatique de Nancy (CRAN)&#39; or in English, Research Center for Automatic Control in Nancy. Further, true to the spirit of a control engineer being problem solver, I have worked in several domains solving technical problems: making telecom electronics systems work, searching for sensor faults inside a nuclear reactor core, putting the smart in smart grids, optimising energy in buildings, finding leaks in water distribution networks and so on. . But all this doesn&#39;t necessarily qualify me, but what qualifies me the most is that I have/had strong opinions and I&#39;m revisiting many of them, either to defend them or to relinquish them for a better one. . Still, what is a post-modern-control-engineer? . It requires a bit of explanation (of course). . Control systems engineering isn&#39;t a traditional engineering discipline such as civil, electrical or mechanical. It is more akin to computer science engineering. Computer science engineering as a discipline grew as computers became complex, not just the electronics that underlie them, but the way a fully-built computer can do millions of different tasks in a myriad number of ways. This necessitated an abstract perspective to understand computers that are neither looking at the underlying electronics nor bothered about how a lay individual used it. To put it simply, a computer science engineer aims to guarantee the computer system would provide stable, optimized performance to attain a specific task (assuming the electronics works fine and the user isn&#39;t inputting garbage). . Like computer science engineering caters to the abstraction of computers, control engineering caters to the abstraction of all engineering systems. The discipline in its nascent form grew during the industrial revolution when the need to develop a stable system that guarantees certain performance became paramount (Steam Engine Governors)). And this leads to people making abstraction of these complex engineering systems. Until around the 1960s, this analysis focused on the frequency domain characteristics of the systems. The real-world system characteristics are written using differential equations and so analysing them in frequency domain meant the use of transforms (Laplace transforms). From the 1960s, the so-called state-space methods ushered in an era where one could directly analyze the differential equations after rewriting them in some nice form. Linear algebra started to run the show of this era referred to as the &#39;modern control era&#39;. . Unlike the revolution in computer science engineering leading to an explosion in the capabilities of a computer, modern control approaches found little takers outside of academia barring isolated successes (e.g., Kalman filter). So much so that many approaches developed in the 1940s are still widely used across many industries. For academics, it feels like walking into a computer server room to find punch cards and vacuum tubes. But for the industrial practitioners, though, the modern control theory is just unintuitive and doesn&#39;t give confidence. . After almost half a decade of the modern control era, with the advent of cheap sensors and a lot of data, the control community was hoping for a change of winds. And the winds are changing. But with it came a new breed of computer science engineers who suddenly repackaged the linear algebra tools that control engineers prized and started applying them to the deluge of internet-fueled data. &quot;Machine Learning is just rehashed Linear Algebra&quot; is a common refrain. It is true, but it also appears a desperate attempt to discredit someone taking away what control engineers have long prized. . As I try to be pragmatic about this situation, my interest is more on the following question . . Note: Would the industry practitioners change in any meaningful way with this? Or would it just be a cosmetic makeup? . Control engineering is entering a post-modern era. In some way, but starting to look at every system through the lens of data, a new discipline that merges the data-charming wizards of computer science engineering and control engineers might emerge. I&#39;m pre-empting and calling them Post-Modern-Control-Engineer (PoMoCoE). . That&#39;s why the name. . Oof, so what will there be? . Join me in the journey of discovery, nerding, nostalgia, rants, feedback control history, keeping in touch with control theory research, most importantly, my attempts to integrate into the AI/ML community (that means childish projects). .",
            "url": "https://krishnans14.github.io/feedback-control/musings/2020/11/23/About-The-Blog.html",
            "relUrl": "/musings/2020/11/23/About-The-Blog.html",
            "date": " • Nov 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Krishnan Srinivasarengan. I’m a control engineer by training and spirit. This blog is an attempt to understand the role of control engineer in the rapidly changing tech world, but also a place to share my thoughts, learnings, etc. . I’ve a PhD in Automatic Control from the Université de Lorraine, Nancy, France, a Masters in Electrical Engineering with the specialisation Control and Computing from the Indian Institute of Technology Bombay, India, and a Bachelors in Instrumentation and Control from National Institute of Technology, Trichy, India. . I live in France1. . at the time of this commit/writing. &#8617; . |",
          "url": "https://krishnans14.github.io/feedback-control/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://krishnans14.github.io/feedback-control/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}