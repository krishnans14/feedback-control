{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Autoencoders: Hope or Hype? - Part I\"\n",
    "> \"The Deeplearning's answer to unsupervised learning?\"\n",
    "\n",
    "- toc:false\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: Krishnan Srinivasarengan\n",
    "- categories: [babysteps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: The *babysteps* series would contain my preliminary explorations into a new area or topic and is prone to  mistakes and misconceptions. You are welcome to comment to make my understanding better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, I participated in a test where I was asked to design an autoencoder. And given that I had barely put my legs into the deep learning domain, I felt a bit overwhelmed by this new idea. Reading a bit about it made me feel nice. Some people claimed it is the DeepLearning's answer to people mocking the need for labelled data to learn. And tons of online tutorials sprang up that would help people to design an autoencoder for dimensionality reduction and anomaly detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Wow\", I thought, \"This should be the saving grace. Is that why this test on anomaly detection asks me to do autoencoders?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of autoencoders is clever and makes it easy to learn things without telling them. So I needed to definitely take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So what's an autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a neural network architecture, or I would say, a neural network *super-architecture*. What do I mean? We have different types of neural network architectures, convolutional, recurrent, and so on. Autoencoders aren't held hostage to one particular type of architecture, rather, they use these types of neural network as building blocks for their architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: An autoencoder is a way to organize neural networks in a particular way to handle special scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those special scenarios? \n",
    "\n",
    "* When labeled data is not available for training. So the algorithm is on its own to perform the exploration.\n",
    "* When we not just not have labeled data, but also want to reduce the dimensions of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How does the autoencoder accomplish this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By a clever architecture. \n",
    "\n",
    "The reason why neural networks require tons of training data is that they need to learn everything about the data from scratch{% fn 1%}. {{ \"I'm talking in general here, and not the fact that transfer learning has changed things a lot for many neural net applications\" | fndetail: 1 }} The neural nets have no clue what the data is all about unless the labels tell them, and this would then help the neural nets to define a loss function to monitor their own performance. What does this monitoring do? Analyse which weight combinations work such that the neural net models the training data well (not considering regularization, etc.)\n",
    "\n",
    "So when there are no labels, but just some input data, the neural net can not monitor its performance. That means,  bad and good results are considered equal. Autoencoders turn this around and ask, what if I can create a mechanism to reproduce the input? And what if I can use this ability to reproduce the input as a way to create a loss function and monitor the performance? \n",
    "\n",
    "The autoencoders achieve this by partitioning the neural net into three parts:\n",
    "\n",
    "* Encoder\n",
    "* Code\n",
    "* Decoder\n",
    "\n",
    "<!--\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png\" alt=\"https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png\" width=\"500\" height=\"300\">\n",
    "-->\n",
    "\n",
    "![Wikipedia Image of Autoencoder Schema](https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png \"Source: wikipedia\")\n",
    "\n",
    "\n",
    "For a control engineer this kind of translates to:\n",
    "\n",
    "* System identification or Parameter Estimation\n",
    "* System Model\n",
    "* System Simulation\n",
    "\n",
    "This parallel excited me, though I was aware of the limitations of the parallel. For one, the three steps are disconnected, so to speak, in control theory. But autoencoders bring about a way to put the three modules together. How does the autoencoder do that? For now, it is magic for me. In simple terms, this is what appears to happen in the autoencoders:\n",
    "\n",
    "1. The encoders (a neural network with possibly progressively reducing the number of units in the subsequent layer) encode the input data focusing on what is important to learn.\n",
    "2. The code is a small dimension neural net layer which forms a condensed model for the input.\n",
    "3. The decoders are essentially encoders in reverse (in terms of architecture, not the weights) that help to reconstruct the input by doing what the encoders did in the reverse order.\n",
    "\n",
    "\n",
    "So autoencoders have solved the labelled training data problem? We can do unsupervised learning with neural nets? So the AI singularity is upon us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though autoencoders seem elegant and powerful, it appears that they are not always classified as part of unsupervised learning algorithms. This is actually confusing because they don't take any inputs and so are technically unsupervised. Some people refer to [autoencoders being part of self-supervised learning](https://blog.keras.io/building-autoencoders-in-keras.html). The linked article is a bit damning on autoencoders.\n",
    "\n",
    "> So what's the big deal with autoencoders?\n",
    ">\n",
    "> Their main claim to fame comes from being featured in many introductory machine learning classes available online. As a result, a lot of newcomers to the field absolutely love autoencoders and can't get enough of them. This is the reason why this tutorial exists!\n",
    "(https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "\n",
    "That seems to make autoencoders a joke. This is a blog on Keras website and though it is dated to 2016 (an epoch in the deeplearning world domain), \n",
    "\n",
    ">Otherwise, one reason why they have attracted so much research and attention is because they have long been thought to be a potential avenue for solving the problem of unsupervised learning, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a self-supervised technique, a specific instance of supervised learning where the targets are generated from the input data.\n",
    "\n",
    "But what is encouraging is that autoencoders were/are a prime candidate for unsupervised learning. So my initial impressions weren't wrong. However, it appears that it went out of favour, though I don't know if the apparent resurgence (well, the only resurgence is the interest my test showed for its use for anomaly detection) is well founded. I have to understand it well to say something about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I did was, I built an autoencoder in Keras/Tensorflow for the test. I tried different internal configurations (Dense, LSTM), but things go as follows (this is a generalized version I created so that one can reuse it for different number of layers/units):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "nb_hidden_layer = 2\n",
    "hidden_layer_size = np.array([50, 25])\n",
    "code_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_model(input_size, nb_hidden_layer=1, hidden_layer_size=np.array([50]), code_size=10):\n",
    "    \"\"\"\n",
    "    This is an autoenconder architecture of a neural network with Dense layers with a number of hoices on\n",
    "    * The number of layers\n",
    "    * Number of units in each layers (including the code unit)\n",
    "    \"\"\"\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    input_unit = layers.Input(shape=(input_size,))\n",
    "    \n",
    "    dummy_unit = input_unit\n",
    "    for i in range(0, nb_hidden_layer):\n",
    "        hidden_unit_encoder = layers.Dense(hidden_layer_size[i], activation='relu')(dummy_unit)\n",
    "        dummy_unit = hidden_unit_encoder\n",
    "        \n",
    "    code_unit = layers.Dense(code_size, activation='relu')(hidden_unit_encoder)\n",
    "    \n",
    "    dummy_unit = code_unit    \n",
    "    for i in range(nb_hidden_layer,0,-1):\n",
    "        hidden_unit_decoder = layers.Dense(hidden_layer_size[i-1], activation='relu')(dummy_unit)\n",
    "        dummy_unit = hidden_unit_decoder\n",
    "    \n",
    "    output_unit = layers.Dense(input_size, activation='sigmoid')(hidden_unit_decoder)\n",
    "    \n",
    "    autoencoder = Model(input_unit, output_unit)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    print(\"------ The following Autoencoder Model was created ------\")\n",
    "    autoencoder.summary()\n",
    "    return autoencoder    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will just try to quickly explain what I did.\n",
    "\n",
    "* First the input layer. It contains the input size/shape (`input_unit`).\n",
    "* Second a set of layers created using the `for` loop: The encoder (`hidden_unit_encoder`)\n",
    "* Third, the code unit (`code_unit`)\n",
    "* Fourth, the reverse of the encoder using a `for` loop (`hidden_unit_decoder`)\n",
    "* Fifth, the output layer. I used `sigmoid` activation because of the need for that application (`output_unit`). \n",
    "\n",
    "Once the neural net is created, I also put them in a specific form that `Keras` allows one to put in. Then, the model is compiled. The following is what I get when I run this function. The `summary()` function gives an idea of the created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ The following Autoencoder Model was created ------\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                496       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                510       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               3100      \n",
      "=================================================================\n",
      "Total params: 7,416\n",
      "Trainable params: 7,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_autoencoder = autoencoder_model(input_size, 2, [30, 16], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did I use this autoencoder for?\n",
    "\n",
    "An unsupervised anomaly detection algorithm for timeseries data.\n",
    "\n",
    "Did I do well?\n",
    "\n",
    "Not sure. But I will share it in another post.\n",
    "\n",
    "What's my hope?\n",
    "\n",
    "I think that autoencoders, when tuned well, can perform a good job in learning signals' characteristics. So if we give inputs with a lot of non-anomalous components and a bit of anomalous components, then the reconstruction error would point out to possible anomalies. This looks well and good, though I also see why this can be very restrictive as it might work only within some region around the input data and can't truly be unsupervised.\n",
    "\n",
    "But application developers don't care if it is technically 'unsupervised' or not, what matters is, for their purpose, it can do well even when input labels are not available. That is perhaps it is interesting for applications in Industry 4.0/ Predictive maintenance, because anomaly detection is central to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
